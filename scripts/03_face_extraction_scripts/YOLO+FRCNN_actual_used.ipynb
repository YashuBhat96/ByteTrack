{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "IDetect.fuse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ybr5070\\AppData\\Local\\anaconda3\\envs\\yolo_env\\Lib\\site-packages\\torch\\functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:3550.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "c:\\Users\\ybr5070\\AppData\\Local\\anaconda3\\envs\\yolo_env\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ybr5070\\AppData\\Local\\anaconda3\\envs\\yolo_env\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: R01_006_V4_PS1_fixed.mp4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import torch\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "\n",
    "# YOLO dependencies\n",
    "sys.path.append(r\"C:\\Users\\ybr5070\\yolov7\")  # Update this to point to your YOLOv7 path\n",
    "from models.experimental import attempt_load\n",
    "from utils.general import non_max_suppression\n",
    "\n",
    "# Define device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Transformation pipeline for resizing images to 224x224\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Save and resize face detection to 224x224 (without the bounding box edges)\n",
    "def save_and_resize_frame(frame, bbox, output_dir, frame_time):\n",
    "    x, y, w, h = [int(v) for v in bbox]\n",
    "    face = frame[y:y+h, x:x+w]  # Crop the face region\n",
    "    if face.size > 0:\n",
    "        face_pil = Image.fromarray(cv2.cvtColor(face, cv2.COLOR_BGR2RGB))\n",
    "        face_resized = transform(face_pil)\n",
    "\n",
    "        # Filename with timestamp in seconds (e.g., tracked_0012.34.png)\n",
    "        timestamp_str = f\"{frame_time:.2f}\"  # Zero padding for uniform filenames\n",
    "        save_path = os.path.join(output_dir, f\"tracked_{timestamp_str}.png\")\n",
    "        \n",
    "        # Save the resized face image\n",
    "        face_resized_pil = transforms.ToPILImage()(face_resized)\n",
    "        face_resized_pil.save(save_path)\n",
    "        \n",
    "        return save_path, frame_time  # Return filename and timestamp for mapping\n",
    "\n",
    "# Load models\n",
    "def initialize_models(yolo_model_path, faster_rcnn_model_path):\n",
    "    yolo_model = attempt_load(yolo_model_path, map_location=device)\n",
    "    faster_rcnn_model = fasterrcnn_resnet50_fpn(pretrained=False, num_classes=2).to(device)\n",
    "    faster_rcnn_model.load_state_dict(torch.load(faster_rcnn_model_path))\n",
    "    \n",
    "    return yolo_model, faster_rcnn_model\n",
    "\n",
    "# Weighted bounding box calculation\n",
    "def dynamic_weighted_bounding_boxes(yolo_bbox, yolo_conf, frcnn_bbox, frame_width, frame_height, size_threshold=0.5):\n",
    "    yolo_area = (yolo_bbox[2] - yolo_bbox[0]) * (yolo_bbox[3] - yolo_bbox[1])\n",
    "    frcnn_area = (frcnn_bbox[2] - frcnn_bbox[0]) * (frcnn_bbox[3] - frcnn_bbox[1])\n",
    "    area_ratio = yolo_area / frcnn_area if frcnn_area > 0 else 0\n",
    "\n",
    "    yolo_weight = 1.0\n",
    "    frcnn_weight = 0.0\n",
    "\n",
    "    if yolo_conf >= 0.2:  # Strong confidence for YOLO\n",
    "        yolo_weight = 1.0\n",
    "        frcnn_weight = 0.0\n",
    "    elif area_ratio < size_threshold:  # Smaller YOLO box, reduce YOLO weight\n",
    "        yolo_weight = 0.4\n",
    "        frcnn_weight = 0.6\n",
    "\n",
    "    x1_avg = int((yolo_bbox[0] * yolo_weight + frcnn_bbox[0] * frcnn_weight))\n",
    "    y1_avg = int((yolo_bbox[1] * yolo_weight + frcnn_bbox[1] * frcnn_weight))\n",
    "    x2_avg = int((yolo_bbox[2] * yolo_weight + frcnn_bbox[2] * frcnn_weight))\n",
    "    y2_avg = int((yolo_bbox[3] * yolo_weight + frcnn_bbox[3] * frcnn_weight))\n",
    "\n",
    "    return (x1_avg, y1_avg, x2_avg - x1_avg, y2_avg - y1_avg)\n",
    "\n",
    "# Process frame for YOLO detection\n",
    "def process_frame_yolo(frame, model, target_size=(512, 320)):  # Updated target size\n",
    "    resized_frame = cv2.resize(frame, target_size)\n",
    "    tensor_frame = F.to_tensor(resized_frame).unsqueeze(0).to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prediction = model(tensor_frame)\n",
    "        prediction = non_max_suppression(prediction[0] if isinstance(prediction, tuple) else prediction, 0.2, 0.8)\n",
    "    return prediction, resized_frame\n",
    "\n",
    "# Process frame for Faster R-CNN detection\n",
    "def process_frame_frcnn(frame, model):\n",
    "    tensor_frame = F.to_tensor(frame).unsqueeze(0).to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prediction = model(tensor_frame)\n",
    "    return prediction\n",
    "\n",
    "# Detect and track faces using YOLO and Faster R-CNN\n",
    "# Detect and track faces using YOLO as the primary and Faster R-CNN as fallback\n",
    "def detect_and_track(video_path, yolo_model, faster_rcnn_model, subject_output_dir, redetect_interval=20, fps=30):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error opening video file: {video_path}\")\n",
    "        return\n",
    "\n",
    "    tracker = cv2.legacy.TrackerKCF_create()\n",
    "    init_tracking = False\n",
    "    frame_count = 0\n",
    "    last_bbox = None\n",
    "    last_confidence = 0.0\n",
    "\n",
    "    # Dictionary to store image file paths and their corresponding timestamps\n",
    "    image_to_timestamp_map = {}\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        original_height, original_width = frame.shape[:2]\n",
    "        frame_time = frame_count / fps  # Calculate time in seconds for current frame\n",
    "\n",
    "        if frame_count % redetect_interval == 0 or not init_tracking:\n",
    "            # YOLO Detection (Primary Model)\n",
    "            yolo_bbox, yolo_conf = None, 0.0\n",
    "            resized_frame = cv2.resize(frame, (512, 320))\n",
    "            yolo_prediction, _ = process_frame_yolo(resized_frame, yolo_model)\n",
    "\n",
    "            if yolo_prediction and len(yolo_prediction[0]) > 0:\n",
    "                element = yolo_prediction[0][0]\n",
    "                yolo_bbox = element[:4].cpu().numpy()\n",
    "                yolo_conf = element[4].item()\n",
    "\n",
    "                scale_x = original_width / 512\n",
    "                scale_y = original_height / 320\n",
    "                yolo_bbox = [int(c * scale_x) if i % 2 == 0 else int(c * scale_y) for i, c in enumerate(yolo_bbox)]\n",
    "                last_bbox = (yolo_bbox[0], yolo_bbox[1], yolo_bbox[2] - yolo_bbox[0], yolo_bbox[3] - yolo_bbox[1])\n",
    "\n",
    "                tracker = cv2.legacy.TrackerKCF_create()\n",
    "                tracker.init(frame, last_bbox)\n",
    "                init_tracking = True\n",
    "                last_confidence = yolo_conf\n",
    "\n",
    "            # Fallback to Faster R-CNN if YOLO fails or confidence is low\n",
    "            frcnn_bbox, frcnn_conf = None, 0.0\n",
    "            if yolo_bbox is None or yolo_conf < 0.2:  # Fallback condition\n",
    "                frcnn_prediction = process_frame_frcnn(frame, faster_rcnn_model)\n",
    "                if len(frcnn_prediction[0]['boxes']) > 0:\n",
    "                    bbox = frcnn_prediction[0]['boxes'][0].cpu().numpy()\n",
    "                    frcnn_bbox = [int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3])]\n",
    "                    frcnn_conf = frcnn_prediction[0]['scores'][0].item()\n",
    "                    last_bbox = (frcnn_bbox[0], frcnn_bbox[1], frcnn_bbox[2] - frcnn_bbox[0], frcnn_bbox[3] - frcnn_bbox[1])\n",
    "\n",
    "                    tracker = cv2.legacy.TrackerKCF_create()\n",
    "                    tracker.init(frame, last_bbox)\n",
    "                    init_tracking = True\n",
    "                    last_confidence = frcnn_conf\n",
    "\n",
    "        # Tracking using KCF for YOLO or fallback Faster R-CNN bounding boxes\n",
    "        if init_tracking:\n",
    "            success, tracked_bbox = tracker.update(frame)\n",
    "            if success:\n",
    "                x, y, w, h = [int(v) for v in tracked_bbox]\n",
    "\n",
    "                # Save the cropped face, regardless of confidence\n",
    "                save_path, timestamp = save_and_resize_frame(frame, tracked_bbox, subject_output_dir, frame_time)\n",
    "                image_to_timestamp_map[save_path] = timestamp  # Store image file path and timestamp\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    # Return the image-to-timestamp mapping\n",
    "    return image_to_timestamp_map\n",
    "\n",
    "# Process videos in a directory\n",
    "def process_videos(directory_path, yolo_model, faster_rcnn_model, output_dir, fps=30):\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.lower().endswith(('.mp4', '.avi', '.mov')):\n",
    "            video_path = os.path.join(directory_path, filename)\n",
    "            subject_name = os.path.splitext(filename)[0]  # Use filename without extension as the subject name\n",
    "            subject_output_dir = os.path.join(output_dir, subject_name)\n",
    "\n",
    "            # Ensure the subject's output directory exists\n",
    "            if not os.path.exists(subject_output_dir):\n",
    "                os.makedirs(subject_output_dir)\n",
    "\n",
    "            print(f\"Processing video: {filename}\")\n",
    "            image_to_timestamp_map = detect_and_track(video_path, yolo_model, faster_rcnn_model, subject_output_dir, fps=fps)        \n",
    "            \n",
    "# Initialize models first\n",
    "yolo_model_path = r\"C:\\Users\\ybr5070\\yolov7\\runs\\train\\exp4\\weights\\best.pt\"\n",
    "faster_rcnn_model_path = r\"C:\\Users\\ybr5070\\Desktop\\HomeBytes\\frcnn_final.pth\"\n",
    "output_dir = r\"C:\\Users\\ybr5070\\Documents\\PS1_face\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Initialize models\n",
    "yolo_model, faster_rcnn_model = initialize_models(yolo_model_path, faster_rcnn_model_path)\n",
    "\n",
    "# Now, process videos after models have been initialized\n",
    "process_videos(r\"C:\\Users\\ybr5070\\Documents\\video\", yolo_model, faster_rcnn_model, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
